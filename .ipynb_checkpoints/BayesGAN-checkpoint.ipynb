{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a17a50d2-6017-4ceb-bd16-bf5858bfa795",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bayesgan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayesgan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesGenerator, BayesDiscriminator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bayesgan'"
     ]
    }
   ],
   "source": [
    "### BayesGAN on FX + EQ Data with Returns (Jupyter Notebook)\n",
    "\n",
    "# 1. Load Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from bayesgan import BayesGenerator, BayesDiscriminator\n",
    "from bayesgan.models import BayesGenerator, BayesDiscriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d512167-9f77-4080-bfcd-6e7a717edbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (680, 12)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Data\n",
    "data = pd.read_csv('../input/raw (FX + EQ).csv', parse_dates=['Date'], index_col='Date')\n",
    "print('Original shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c1e5d13-22fe-43a1-bbf1-6183fdf51f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to log returns\n",
    "data_returns = np.log(data / data.shift(1)).dropna()\n",
    "\n",
    "# Normalize Data between -1 and 1\n",
    "data_min = data_returns.min()\n",
    "data_max = data_returns.max()\n",
    "data_norm = 2 * (data_returns - data_min) / (data_max - data_min) - 1\n",
    "\n",
    "X_real = torch.tensor(data_norm.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7478fdc4-4da3-4f36-9e39-6216435da16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create Dataset and DataLoader\n",
    "dataset = TensorDataset(X_real)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f78f367f-3902-4ee5-9e0a-93d6bb6d3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Standard Generator and Discriminator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, data_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, data_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a98331-b79a-42cd-a1fa-55afc5fe01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100  # or whatever you were using\n",
    "input_dim = 12    # your data input features\n",
    "\n",
    "G = BayesGenerator(z_dim=latent_dim, x_dim=input_dim)\n",
    "D = BayesDiscriminator(x_dim=input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99277657-6df0-4b03-ae56-4e499db1725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dcb585d-bbbc-4176-bd74-5cddf95ce084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SGLD Optimizer (basic version)\n",
    "class SGLD(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, noise_scale=1e-4):\n",
    "        defaults = dict(lr=lr, noise_scale=noise_scale)\n",
    "        super(SGLD, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            noise_scale = group['noise_scale']\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    d_p = p.grad\n",
    "                    noise = torch.randn_like(p) * (2 * lr * noise_scale)**0.5\n",
    "                    p.add_(noise - lr * d_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feba479-d4f9-422f-b719-b56a2d3ea01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for real_batch in data_loader:\n",
    "        batch_size = real_batch.size(0)\n",
    "        \n",
    "        # Labels\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        \n",
    "        # Move to device if needed\n",
    "        real_batch = real_batch.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "        fake_labels = fake_labels.to(device)\n",
    "\n",
    "        # ================\n",
    "        # Train Discriminator\n",
    "        # ================\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        # Generate fake data\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_data = G(z)\n",
    "        \n",
    "        # Discriminator outputs\n",
    "        D_real = D(real_batch)\n",
    "        D_fake = D(fake_data.detach())\n",
    "        \n",
    "        # Discriminator loss\n",
    "        D_loss_real = loss_fn(D_real, real_labels)\n",
    "        D_loss_fake = loss_fn(D_fake, fake_labels)\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        \n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # ================\n",
    "        # Train Generator\n",
    "        # ================\n",
    "        G_optimizer.zero_grad()\n",
    "\n",
    "        # Generate fake data again\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_data = G(z)\n",
    "\n",
    "        D_fake = D(fake_data)\n",
    "        \n",
    "        # Generator loss\n",
    "        G_loss = loss_fn(D_fake, real_labels)  # want fake to be classified as real\n",
    "        \n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "    # Print losses\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | D Loss: {D_loss.item():.4f} | G Loss: {G_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca67a4c-0567-4116-a216-50cca084ca85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c91e093-08c1-4f22-a911-c957694c42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Instantiate Models\n",
    "noise_dim = 20\n",
    "G = Generator(noise_dim=noise_dim, data_dim=12)\n",
    "D = Discriminator(data_dim=12)\n",
    "\n",
    "G_optimizer = SGLD(G.parameters(), lr=1e-4, noise_scale=1e-4)\n",
    "D_optimizer = SGLD(D.parameters(), lr=1e-4, noise_scale=1e-4)\n",
    "\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a63bc2f-5edf-4539-a450-534497447683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300 | D Loss: 1.3837 | G Loss: 0.6909\n",
      "Epoch 100/300 | D Loss: 1.3755 | G Loss: 0.6905\n",
      "Epoch 150/300 | D Loss: 1.3685 | G Loss: 0.6868\n",
      "Epoch 200/300 | D Loss: 1.3681 | G Loss: 0.6881\n",
      "Epoch 250/300 | D Loss: 1.3637 | G Loss: 0.6902\n",
      "Epoch 300/300 | D Loss: 1.3562 | G Loss: 0.6862\n"
     ]
    }
   ],
   "source": [
    "# 7. Train BayesGAN\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    for real_batch, in dataloader:\n",
    "        batch_size = real_batch.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        z = torch.randn(batch_size, noise_dim)\n",
    "        fake_data = G(z).detach()\n",
    "\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "        D_real = D(real_batch)\n",
    "        D_fake = D(fake_data)\n",
    "\n",
    "        D_loss = loss_fn(D_real, real_labels) + loss_fn(D_fake, fake_labels)\n",
    "\n",
    "        D_optimizer.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, noise_dim)\n",
    "        generated_data = G(z)\n",
    "        D_generated = D(generated_data)\n",
    "\n",
    "        G_loss = loss_fn(D_generated, real_labels)\n",
    "\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | D Loss: {D_loss.item():.4f} | G Loss: {G_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c26d2-2a2c-40fd-a8ed-da0b56b8c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Generate 1000 synthetic samples\n",
    "G.eval()\n",
    "z = torch.randn(1000, noise_dim)\n",
    "synthetic_samples = G(z).detach().numpy()\n",
    "\n",
    "# De-normalize synthetic returns\n",
    "synthetic_returns = (synthetic_samples + 1) / 2 * (data_max.values - data_min.values) + data_min.values\n",
    "\n",
    "synthetic_returns_df = pd.DataFrame(synthetic_returns, columns=data.columns)\n",
    "print(synthetic_returns_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593fea2-2f8c-40cd-97dd-9d263b23b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Rebuild synthetic prices from returns\n",
    "last_real_price = data.iloc[-1]\n",
    "synthetic_prices = [last_real_price.values]\n",
    "\n",
    "for ret in synthetic_returns_df.values:\n",
    "    next_price = synthetic_prices[-1] * np.exp(ret)\n",
    "    synthetic_prices.append(next_price)\n",
    "\n",
    "synthetic_prices = np.array(synthetic_prices[1:])\n",
    "synthetic_prices_df = pd.DataFrame(synthetic_prices, columns=data.columns)\n",
    "\n",
    "print(synthetic_prices_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde5423-3e44-4cb2-9fce-d6423f61e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Forecasting (simple generation conditioned on last real point)\n",
    "last_real = X_real[-1]\n",
    "forecast_steps = 10\n",
    "\n",
    "forecast = []\n",
    "current_input = last_real.unsqueeze(0)\n",
    "\n",
    "for _ in range(forecast_steps):\n",
    "    noise = torch.randn(1, noise_dim)\n",
    "    next_return = G(noise)\n",
    "    forecast.append(next_return.squeeze(0).detach().numpy())\n",
    "\n",
    "forecast = np.array(forecast)\n",
    "forecast_returns = (forecast + 1) / 2 * (data_max.values - data_min.values) + data_min.values\n",
    "\n",
    "# Rebuild forecasted prices\n",
    "forecast_prices = [last_real_price.values]\n",
    "for ret in forecast_returns:\n",
    "    next_price = forecast_prices[-1] * np.exp(ret)\n",
    "    forecast_prices.append(next_price)\n",
    "\n",
    "forecast_prices = np.array(forecast_prices[1:])\n",
    "forecast_df = pd.DataFrame(forecast_prices, columns=data.columns)\n",
    "\n",
    "print(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7e575-c26a-4d53-9096-11e235264dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Visualization\n",
    "plt.figure(figsize=(12,6))\n",
    "for i in range(5):\n",
    "    plt.hist(synthetic_returns_df.iloc[:, i], bins=30, alpha=0.5, label=f'Synthetic {data.columns[i]}')\n",
    "    plt.hist(data_returns.iloc[:, i], bins=30, alpha=0.5, label=f'Real {data.columns[i]}')\n",
    "    plt.title(f'Return Distribution for {data.columns[i]}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a235a94-b606-4bd7-b4bc-17d650931643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Correlation Matrix Comparison\n",
    "real_corr = data_returns.corr()\n",
    "synth_corr = synthetic_returns_df.corr()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "axes[0].imshow(real_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[0].set_title('Real Data Correlation Matrix')\n",
    "axes[1].imshow(synth_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Synthetic Data Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2cbf1-583b-4254-b620-9681e86652b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. PCA Plot\n",
    "pca = PCA(n_components=2)\n",
    "real_pca = pca.fit_transform(data_returns)\n",
    "synth_pca = pca.fit_transform(synthetic_returns_df)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(real_pca[:,0], real_pca[:,1], alpha=0.5, label='Real Returns')\n",
    "plt.scatter(synth_pca[:,0], synth_pca[:,1], alpha=0.5, label='Synthetic Returns')\n",
    "plt.title('PCA Comparison of Real vs Synthetic Returns')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
